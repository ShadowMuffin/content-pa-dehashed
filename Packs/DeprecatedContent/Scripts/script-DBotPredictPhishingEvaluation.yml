args:
- defaultValue: created:>="3 days ago" and type:"Phishing"
  description: Query of the phishing incidents.
  name: incidentsQuery
- defaultValue: '100'
  description: Maximum number of incidents
  name: maxNumberOfIncidents
- defaultValue: details
  description: Incident key to extract email text
  name: emailTextKey
- defaultValue: name
  description: Incident key to extract email subject
  name: emailSubjectKey
- defaultValue: closeReason
  description: Incident key to extract tag
  name: tagKey
- defaultValue: '*'
  description: 'Comma-separated values of email tags values and mapping. The script
    going to consider only the tags specify in this field. You can map label to another
    value by using this format: LABEL:MAPPED_LABEL. For example: let''s say we have
    4 values in email tag: malicious, credentials harvesting, inner communitcation,
    external legit email, unclassified. While training, we want to ignore "unclassified"
    tag, and refer to "credentials harvesting" as "malicious" too. Also, we want to
    merge "inner communitcation" and "external legit email" to one tag called "non-malicious".
    The input will be: malicious, credentials harvesting:malicious, inner communitcation:non-malicious,
    external legit email:non-malicious'
  name: phishingLabels
- auto: PREDEFINED
  defaultValue: 'no'
  description: Does one of the fields is in the context data?
  name: isContextNeeded
  predefined:
  - 'yes'
  - 'no'
- auto: PREDEFINED
  defaultValue: 'no'
  description: Is phishing model based on hashed data?
  name: hashData
  predefined:
  - 'yes'
  - 'no'
- description: Demisto list name that stores the machine learning model
  name: modelListName
  required: true
comment: This script is deprecated. See https://xsoar.pan.dev/docs/reference/playbooks/d-bot-create-phishing-classifier-v2
  for more information.
commonfields:
  id: DBotPredictPhishingEvaluation
  version: -1
deprecated: true
dockerimage: demisto/dl:1.1
enabled: true
fromversion: 4.1.0
name: DBotPredictPhishingEvaluation
outputs:
- contextPath: DBotPredictPhishingEvaluation.Precision
  description: Precision score (0-1)
  type: number
- contextPath: DBotPredictPhishingEvaluation.Recall
  description: Recall score (0-1)
  type: number
- contextPath: DBotPredictPhishingEvaluation.F1
  description: F1 score (0-1)
  type: number
- contextPath: DBotPredictPhishingEvaluation.Size
  description: Test data size
  type: number
runonce: true
script: "import pandas as pd\nfrom sklearn import metrics\nimport sys\n\n# disable\
  \ warnings\nimport warnings\nwarnings.filterwarnings('always')\n\ndef get_threshold_precision(df,\
  \ threshold, predicted_col, original_col, threshold_col):\n    skipped_count = len(df[df[threshold_col]\
  \ < threshold])\n    if skipped_count == len(df):\n        percentage_skipped =\
  \ 100\n        precision_score = 0\n        recall_score = 0\n        f1_score =\
  \ 0\n    else:\n        precision_score = metrics.precision_score(df[df[threshold_col]\
  \ >= threshold][original_col], df[df[threshold_col] >= threshold][predicted_col],\
  \ average='micro') * 100\n        recall_score = metrics.recall_score(df[df[threshold_col]\
  \ >= threshold][original_col], df[df[threshold_col] >= threshold][predicted_col],\
  \ average='micro') * 100\n        f1_score = metrics.f1_score(df[df[threshold_col]\
  \ >= threshold][original_col], df[df[threshold_col] >= threshold][predicted_col],\
  \ average='micro') * 100\n        percentage_skipped = (float(skipped_count) / len(df))\
  \ * 100\n\n    return {\n        'Threshold': threshold,\n        'Precision': '%.2f%%'\
  \ % precision_score,\n        'Recall': '%.2f%%' % recall_score,\n        'Below\
  \ Threshold': '%d (%.2f%%)' % (skipped_count, percentage_skipped),\n        'Count':\
  \ '%d (%.2f%%)' % (len(df) - skipped_count, 100 - percentage_skipped),\n       \
  \ 'F1 Score': '%.2f%%' % f1_score,\n    }\n\nentries = demisto.executeCommand('DBotPreparePhishingData',\
  \ demisto.args())\nif isError(entries[0]):\n    return_error(entries[0]['Contents'])\n\
  else:\n    entries = [e for e in entries if e['ContentsFormat'] == formats['json']]\n\
  \    labeled_data = entries[0]['Contents']\nif len(labeled_data) == 0:\n    return_error(\"\
  there is no data to evaluate\")\nmax_number_of_incidents = int(demisto.args()['maxNumberOfIncidents'])\n\
  if len(labeled_data) == max_number_of_incidents:\n    demisto.log('Results from\
  \ incidentsQuery has reached to the maximum (maxNumberOfIncidents=%d). Consider\
  \ increasing maxNumberOfIncidents to get better results' % max_number_of_incidents)\n\
  demisto.log(\"Evaluating %d samples\" % len(labeled_data))\nres = demisto.executeCommand('DBotPredictTextLabel',\
  \ {\n    'inputText': map(lambda x: x['txt'], labeled_data),\n    'modelListName':\
  \ demisto.args()['modelListName']\n})\nif isError(res[0]):\n    return_error(res[0]['Contents'])\n\
  \npredictions = res[0]['Contents']\n\ndf = pd.DataFrame.from_dict(predictions)\n\
  df.loc[:, 'Original'] = pd.Series(map(lambda x: x['label'], labeled_data))\n\nresult\
  \ = []\nfor i in xrange(60, 80, 10):\n    result.append(get_threshold_precision(df,\
  \ float(i)/100, 'Label', 'Original', 'Probability'))\nfor i in xrange(80, 100, 5):\n\
  \    result.append(get_threshold_precision(df, float(i)/100, 'Label', 'Original',\
  \ 'Probability'))\n\ntable_headers = ['Threshold', 'Below Threshold', 'Count', 'F1\
  \ Score', 'Precision', 'Recall']\nhr1 = tableToMarkdown('Evaluation probablity with\
  \ different thresholds', result, headers=table_headers)\nresult = []\nfor i in xrange(0,\
  \ 110, 10):\n    result.append(get_threshold_precision(df, i, 'Label', 'Original',\
  \ 'InputTextNumberOfTokens'))\n\nhr2 = tableToMarkdown('Evaluation input text length\
  \ with different thresholds', result, headers=table_headers)\nprecision_score =\
  \ metrics.precision_score(df['Original'], df['Label'], average='micro')\nrecall_score\
  \ = metrics.recall_score(df['Original'], df['Label'], average='micro')\nf1_score\
  \ = metrics.f1_score(df['Original'], df['Label'], average='micro')\neval_result\
  \ = {\n    'Precision': precision_score,\n    'Recall': recall_score,\n    'F1':\
  \ f1_score,\n    'Size': len(labeled_data)\n}\ndemisto.results({\n    'ContentsFormat':\
  \ formats['json'],\n    'Type': entryTypes['note'],\n    'Contents': eval_result,\n\
  \    'EntryContext': {\n        'DBotPredictPhishingEvaluation': eval_result\n \
  \   },\n    'HumanReadable': hr1 + \"\\n\\n\" + hr2,\n    'HumanReadableFormat':\
  \ formats[\"markdown\"]\n})\ndetails_string = 'F1:%.2f | Precision:%.2f | Recall:%.2f\
  \ | SamplesCount: %d' % (f1_score, precision_score, recall_score, len(labeled_data))\n\
  demisto.executeCommand(\"setIncident\", {'details': details_string})\n\n"
scripttarget: 0
subtype: python2
tags:
- ml
- phishing
tests:
- No tests
timeout: "6\xB5s"
toversion: 4.1.9
type: python
