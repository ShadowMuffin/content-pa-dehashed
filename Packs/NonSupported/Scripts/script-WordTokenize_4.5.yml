args:
- description: The text value
  isArray: true
  name: value
  required: true
- auto: PREDEFINED
  defaultValue: word
  description: 'Tokenizer type. you can read more about this here: https://www.nltk.org/api/nltk.tokenize.html'
  name: type
  predefined:
  - word
  - punkt
- auto: PREDEFINED
  defaultValue: 'yes'
  description: Clean html from text value?
  name: cleanHtml
  predefined:
  - 'yes'
  - 'no'
- auto: PREDEFINED
  defaultValue: 'yes'
  description: Remove line breaks?
  name: removeLineBreaks
  predefined:
  - 'yes'
  - 'no'
- defaultValue: utf-8
  description: Text encoding
  name: encoding
- description: If non-empty hash the words with this seed.
  name: hashWordWithSeed
comment: Tokenize the words in a input text.
commonfields:
  id: WordTokenizer
  version: -1
dockerimage: demisto/nltk:1.0
enabled: true
name: WordTokenizer
outputs:
- contextPath: WordTokenizeOutput
  description: 'Output text '
  type: string
runonce: true
script: "import nltk, re\nfrom HTMLParser import HTMLParser\nhtml_parser = HTMLParser()\n\
  \nCLEAN_HTML = (demisto.args()['cleanHtml'] == 'yes')\nREMOVE_LINE_BREAKS = (demisto.args()['removeLineBreaks']\
  \ == 'yes')\nTOKENIZE_TYPE = demisto.args()['type']\nTEXT_ENCODE = demisto.args()['encoding']\n\
  HASH_SEED = demisto.args().get('hashWordWithSeed')\n\nREMOVE_HTML_PATTERNS = [\n\
  \    re.compile(r\"(?is)<(script|style).*?>.*?(</\\1>)\"),\n    re.compile(r\"(?s)<!--(.*?)-->[\\\
  n]?\"),\n    re.compile(r\"(?s)<.*?>\"),\n    re.compile(r\"&nbsp;\"),\n    re.compile(r\"\
  \ +\")\n]\ndef clean_html(text):\n    if not CLEAN_HTML:\n        return text\n\n\
  \    cleaned = text\n    for pattern in REMOVE_HTML_PATTERNS:\n        cleaned =\
  \ pattern.sub(\" \", cleaned)\n    return html_parser.unescape(cleaned).strip()\n\
  \ndef tokenize_text(text):\n    if not text:\n        return ''\n    text = text.lower()\n\
  \    if TOKENIZE_TYPE == 'word':\n        word_tokens = nltk.word_tokenize(text)\n\
  \    elif TOKENIZE_TYPE == 'punkt':\n        word_tokens = nltk.wordpunct_tokenize(text)\n\
  \    else:\n        raise Exception(\"Unsupported tokenize type: %s\" % tokenize_type)\n\
  \    if HASH_SEED:\n        word_tokens = map(str, map(lambda x: hash_djb2(x, int(HASH_SEED)),\
  \ word_tokens))\n    return (' '.join(word_tokens)).encode(TEXT_ENCODE).strip()\n\
  \n\ndef remove_line_breaks(text):\n    if not REMOVE_LINE_BREAKS:\n        return\
  \ text\n\n    return text.replace(\"\\r\",\"\").replace(\"\\n\",\"\")\n\ntext =\
  \ demisto.args()['value']\n\nif type(text) is not list:\n    text = [text]\nresult\
  \ = map(remove_line_breaks, map(tokenize_text, map(clean_html, text)))\n\nif len(result)\
  \ == 1:\n    result = result[0]\n\ndemisto.results({\n    'Contents': result,\n\
  \    'ContentsFormat': formats['json'] if type(result) is list else formats['text'],\n\
  \    'EntryContext': {\n        'WordTokenizeOutput': result\n    }\n})"
scripttarget: 0
subtype: python2
tags:
- phishing
- ml
tests:
- WordTokenizeTest
toversion: 4.1.9
type: python
